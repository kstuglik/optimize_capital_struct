{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/hugosjoberg/house-prices-prediction-using-keras\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# from sklearn.preprocessing import StandardScaler # Used for scaling of data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras import metrics\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from keras import backend as K\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# # if dim == 29:\n",
    "# #     df_train = pd.read_csv(\"../xlsx/combine_vector.csv\", names=['P0','P1','P2','P3','P4','P5','P6','P7','P8','P9','P10','P11','P12','P13','P14','P15','P16','P17','P18','P19','P20','P21','P22','P23','P24','P25','P26','P27','P28','P29'], header=None)\n",
    "    \n",
    "# # #     X = df_train.drop('P29', axis=1)\n",
    "# # #     y = df_train['P29']\n",
    "\n",
    "# # if dim == 58:\n",
    "# df_train = pd.read_csv(\"../xlsx/combine_pasywa_vector.csv\", names=[\\\n",
    "#                                                                   'P0','P1','P2','P3','P4','P5','P6','P7',\n",
    "#                                                                   'P8','P9','P10','P11','P12','P13','P14','P15',\n",
    "#                                                                   'P16','P17','P18','P19','P20','P21','P22','P23',\n",
    "#                                                                   'P24','P25','P26','P27','P28','P29','P30','P31',\n",
    "#                                                                   'P32','P33','P34','P35','P36','P37','P38','P39',\n",
    "#                                                                   'P40','P41','P42','P43','P44','P45','P46','P47',\n",
    "#                                                                   'P48','P49','P50','P51','P52','P53','P54','P55',\n",
    "#                                                                   'P56','P57','P58'], header=None)\n",
    "# #     X = df_train.drop('P58', axis=1)\n",
    "# #     y = df_train['P58']\n",
    "    \n",
    "# # print(df_train.head(10))\n",
    "\n",
    "# df_train = df_train.fillna(0)\n",
    "\n",
    "# # X = df_train.loc[:, df_train.columns != 'P29']\n",
    "# # y = df_train.loc[:, df_train.columns == 'P29']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(df_train['P58'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Skewness: %f\" % df_train['P58'].skew())\n",
    "# print(\"Kurtosis: %f\" % df_train['P58'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrmat = df_train.corr()\n",
    "# f, ax = plt.subplots(figsize=(15, 12))\n",
    "# sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #market value correlation matrix\n",
    "# f, ax = plt.subplots(figsize=(15, 12))\n",
    "# k = 15 #number of variables for heatmap\n",
    "# cols = corrmat.nlargest(k, 'P58')['P58'].index\n",
    "# cm = np.corrcoef(df_train[cols].values.T)\n",
    "# sns.set(font_scale=1)\n",
    "# hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 6}, yticklabels=cols.values,linewidths=0.1, xticklabels=cols.values)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train.fillna(df_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardizing data\n",
    "# marketprice_scaled = StandardScaler().fit_transform(df_train['P58'][:,np.newaxis]);\n",
    "# low_range = marketprice_scaled[marketprice_scaled[:,0].argsort()][:10]\n",
    "# high_range= marketprice_scaled[marketprice_scaled[:,0].argsort()][-10:]\n",
    "# print('outer range (low) of the distribution:')\n",
    "# print(low_range)\n",
    "# print('\\nouter range (high) of the distribution:')\n",
    "# print(high_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.preprocessing import StandardScaler # Used for scaling of data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import optimizers\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ded860e9582c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../xlsx/combine_pasywa_vector.csv\", names=[\\\n",
    "                                                                  'P0','P1','P2','P3','P4','P5','P6','P7','P8',\n",
    "                                                                  'P9','P10','P11','P12','P13','P14','P15','P16',\n",
    "                                                                  'P17','P18','P19','P20','P21','P22','P23','P24',\n",
    "                                                                  'P25','P26','P27','P28','P29','P30','P31','P32',\n",
    "                                                                  'P33','P34','P35','P36','P37','P38','P39','P40',\n",
    "                                                                  'P41','P42','P43','P44','P45','P46','P47','P48',\n",
    "                                                                  'P49','P50','P51','P52','P53','P54','P55','P56',\n",
    "                                                                  'P57','P58'], header=None)\n",
    "# cols = ['P0','P1','P2','P3','P4','P5','P6','P7','P8','P9','P10','P11','P12','P13','P14','P15','P16','P17',\\\n",
    "#         'P18','P19','P20','P21','P22','P23','P24','P25','P26','P27','P28','P29','P30','P31','P32','P33','P34',\n",
    "#         'P35','P36','P37','P38','P39','P40','P41','P42','P43','P44','P45','P46','P47','P48','P49','P50','P51',\n",
    "#         'P52','P53','P54','P55','P56','P57','P58']\n",
    "\n",
    "# df_train = df_train[cols]\n",
    "\n",
    "# Create dummy values\n",
    "df_train = pd.get_dummies(df_train)\n",
    "#filling NA's with the mean of the column:\n",
    "df_train = df_train.fillna(df_train.mean())\n",
    "\n",
    "X_train = df_train[['P0','P1','P2','P3','P4','P5','P6','P7','P8','P9','P10','P11','P12','P13',\\\n",
    "                    'P14','P15','P16','P17','P18','P19','P20','P21','P22','P23','P24','P25','P26',\n",
    "                    'P27','P28','P29','P30','P31','P32','P33','P34','P35','P36','P37','P38','P39',\n",
    "                    'P40','P41','P42','P43','P44','P45','P46','P47','P48','P49','P50','P51','P52',\n",
    "                    'P53','P54','P55','P56','P57']]\n",
    "\n",
    "\n",
    "# distributions = [\n",
    "#     ('Unscaled data', X),\n",
    "#     ('Data after standard scaling',\n",
    "#         StandardScaler().fit_transform(X)),\n",
    "#     ('Data after min-max scaling',\n",
    "#         MinMaxScaler().fit_transform(X)),\n",
    "#     ('Data after max-abs scaling',\n",
    "#         MaxAbsScaler().fit_transform(X)),\n",
    "#     ('Data after robust scaling',\n",
    "#         RobustScaler(quantile_range=(25, 75)).fit_transform(X)),\n",
    "#     ('Data after power transformation (Yeo-Johnson)',\n",
    "#      PowerTransformer(method='yeo-johnson').fit_transform(X)),\n",
    "#     ('Data after power transformation (Box-Cox)',\n",
    "#      PowerTransformer(method='box-cox').fit_transform(X)),\n",
    "#     ('Data after quantile transformation (gaussian pdf)',\n",
    "#         QuantileTransformer(output_distribution='normal')\n",
    "#         .fit_transform(X)),\n",
    "#     ('Data after quantile transformation (uniform pdf)',\n",
    "#         QuantileTransformer(output_distribution='uniform')\n",
    "#         .fit_transform(X)),\n",
    "#     ('Data after sample-wise L2 normalizing',\n",
    "#         Normalizer().fit_transform(X)),\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# Always standard scale the data before using NNMaxAbsScaler\n",
    "X_train = MaxAbsScaler().fit_transform(X_train)\n",
    "\n",
    "\n",
    "\n",
    "y = df_train['P58'].values\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=58, activation='tanh'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(8, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='tanh'))\n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    # Compile model\n",
    "    \n",
    "    model.compile(\n",
    "#         optimizer ='adam', \n",
    "        loss = 'categorical_crossentropy', \n",
    "        metrics =['acc'],\n",
    "#         loss='mse', # Cross-entropy\n",
    "    optimizer='rmsprop', # Root Mean Square Propagation\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "w = [12,8,2,1]\n",
    "# w = [12,8,1]\n",
    "suma = 0\n",
    "for i in range(len(w)):\n",
    "    if i == 0:\n",
    "        suma += w[0]*59\n",
    "    elif i != len(w)-1:\n",
    "        suma += w[i]*(w[i-1]+1)\n",
    "    else:\n",
    "        suma += w[i-1]+w[i]\n",
    "#     print(suma)\n",
    "        \n",
    "srednia = suma/1228\n",
    "wycznacz_podzial =  round(1-srednia,2)*0.2\n",
    "\n",
    "\n",
    "print(suma)\n",
    "print(wycznacz_podzial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=wycznacz_podzial, random_state=seed)\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=20)]\n",
    "#     ModelCheckpoint(filepath='best_model.h5', monitor='accuracy', save_best_only=True)]\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    validation_data=(X_test,y_test), \n",
    "    epochs=400, \n",
    "    batch_size=50,\n",
    "    callbacks=callbacks,\n",
    "#     class_weight=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "scores = model.evaluate(X_train, y_train, verbose=0) \n",
    "print(scores)\n",
    "print(\"acc: %.2f%%\" % (scores[1]*100))\n",
    "scores = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(scores)\n",
    "print(\"acc: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "test_data = np.array([1.4269,0.0000,0.0000,6.3156,0.0000,-12.2964,15.7707,0.0000,0.0000,30.5627,0.0000,0.0000,0.0000,2.5386,0.0000,0.0000,0.0000,0.0000,0.0000,4.6097,0.0000,0.0000,50.6423,0.1912,0.0000,0.2388,0.0000,0.0000,0.0000,1.1984,0.0000,0.0000,1.1970,0.0000,-0.1155,-12.6190,0.0000,0.0000,-8.0022,0.0000,0.0000,0.0000,7.0157,0.0000,0.0000,0.0000,0.0000,0.0000,18.4047,0.0000,0.0000,1.4375,25.2092,0.0000,135.7621,0.0000,0.0000,0.0000])\n",
    "test_data2 = np.array([1.6722,0.0000,-0.7812,27.2519,0.0000,0.0000,32.7353,0.0000,0.0000,2.5775,0.0000,0.0000,0.0000,0.2149,0.0000,2.1944,2.1337,0.0000,0.0000,1.6850,0.0000,0.0000,21.7058,4.8099,0.0000,1.1530,2.6477,0.0000,0.0000,-27.5505,0.0000,27.5474,-27.5522,0.0000,0.0000,-27.0744,0.0000,0.0000,-5.6760,0.0000,0.0000,0.0000,-16.1936,0.0000,-45.6252,-43.2488,0.0000,0.0000,9.9941,0.0000,0.0000,99.6342,-29.2064,0.0000,-45.2905,-47.0522,0.0000,0.0000])\n",
    "test_data3 = np.array([30.2288,0.0000,0.0000,51.7629,-0.0215,2.1568,-40.4181,0.0000,0.0000,16.8416,0.0000,0.0293,0.0000,1.8614,2.6048,0.6720,0.0000,0.0000,0.0000,2.1321,0.0000,0.0869,13.9525,1.5124,2.8103,6.0151,7.7726,0.0000,0.0000,48.4131,0.0000,0.0000,48.4134,-1035.3488,48.4097,-40.4730,0.0000,0.0000,-48.2638,0.0000,0.0000,0.0000,55.2165,42.0915,33.6905,0.0000,0.0000,0.0000,-62.8770,0.0000,-34.9827,-44.3053,38.8720,-50.0160,-51.5453,-94.2426,0.0000,0.0000])\n",
    "test_data4 = np.array([41.0464,0.0000,0.0000,82.2618,0.0000,0.0000,-125.4647,0.0000,0.0000,83.5618,0.0000,0.0000,0.0000,0.0527,0.3210,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,5.6595,0.1899,0.0000,0.0172,12.1142,0.2402,0.0000,-37.0052,0.0000,0.0000,-37.0050,0.0000,0.0000,65.2752,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,-94.1176,1488.1931,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,878.0369,-69.0890,0.0000,0.0000,-57.9931,-60.4913,0.0000])\n",
    "test_data5 = np.array([2.4361,0.0000,-0.0424,46.4519,4.0558,0.0000,2.5693,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,4.8338,0.0000,0.0000,12.2636,0.0000,0.0000,17.7043,1.7062,1.9115,3.1406,0.8036,2.1657,0.0000,-11.4117,0.0000,11.5566,-2.1732,-11.9212,0.0000,-172.8720,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,0.0000,-31.3004,0.0000,0.0000,9.6880,0.0000,0.0000,36.8956,-1.8579,38.5352,-16.2803,1.2568,-9.3780,0.0000])\n",
    "\n",
    "expected = [-2.1,-27,-30,80,64]\n",
    "diff = 0\n",
    "received = []\n",
    "i=0\n",
    "avg = 0\n",
    "\n",
    "\n",
    "for data in [test_data,test_data2,test_data3,test_data4,test_data5]:\n",
    "    received=model.predict(data.reshape(1,58),batch_size =1)[0][0]\n",
    "    print('EXPECTED:%10.4f\\tRECEIVED:%10.4f'%(expected[i],received))\n",
    "    avg += diff\n",
    "    i+=1\n",
    "\n",
    "avg /= 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
